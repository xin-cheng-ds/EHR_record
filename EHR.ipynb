{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a62e09a-b16a-47cf-bc2a-83a764a4f698",
   "metadata": {
    "id": "8a62e09a-b16a-47cf-bc2a-83a764a4f698",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TIehMrYcjCwX",
   "metadata": {
    "id": "TIehMrYcjCwX"
   },
   "outputs": [],
   "source": [
    "#for colab reading files\n",
    "path = 'drive/My Drive/Projects/EHR_record/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vRN8rZYGjsmp",
   "metadata": {
    "id": "vRN8rZYGjsmp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "path=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160c1570-b05e-4bf8-a57c-f75d329e1ea3",
   "metadata": {
    "id": "160c1570-b05e-4bf8-a57c-f75d329e1ea3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "demographic_train = pd.read_csv(path+'Train/demographics.csv')\n",
    "labs_train = pd.read_csv(path+'Train/labs.csv')\n",
    "vitals_train = pd.read_csv(path+'Train/vitals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G_32CmBHi56R",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_32CmBHi56R",
    "outputId": "60a5ef76-9d56-4a9f-8fd0-bc46cd91a24e"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b1088e-b2aa-40e2-af67-14a4e419d25a",
   "metadata": {
    "id": "e0b1088e-b2aa-40e2-af67-14a4e419d25a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "demographic_test = pd.read_csv(path+'Test/demographics.csv')\n",
    "labs_test = pd.read_csv(path+'Test/labs.csv')\n",
    "vitals_test = pd.read_csv(path+'Test/vitals.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116c470f-6f60-4181-bb78-caa467b4b29d",
   "metadata": {
    "id": "116c470f-6f60-4181-bb78-caa467b4b29d"
   },
   "source": [
    "## 1. Exloratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664dc8c8-4aa2-4916-948b-62bd78dfee75",
   "metadata": {
    "id": "664dc8c8-4aa2-4916-948b-62bd78dfee75",
    "outputId": "a20ca1d1-28c8-4e94-b47d-3ace9aaba894",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(demographic_train.describe())\n",
    "print(labs_train.describe())\n",
    "print(vitals_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d892aca1-dcd1-4f1f-80a1-19ba5711281f",
   "metadata": {
    "id": "d892aca1-dcd1-4f1f-80a1-19ba5711281f",
    "outputId": "6e19d276-f730-4186-8e20-c6f59dc3f21f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"size of demographic:\", demographic_train.shape)\n",
    "print(\"size of vitals:\", vitals_train.shape)\n",
    "print(\"size of labs:\", labs_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7579f723-174b-4eba-8d17-bdf85c23e27d",
   "metadata": {
    "id": "7579f723-174b-4eba-8d17-bdf85c23e27d",
    "outputId": "ac1f41a6-cb4d-46ba-e100-df9f74e15081",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Show null values in a dataframe\n",
    "print(demographic_train.isnull().sum())\n",
    "print(labs_train.isnull().sum())\n",
    "print(vitals_train.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3676d8a6-3ba0-4e04-8fb2-89c50e52bd56",
   "metadata": {
    "id": "3676d8a6-3ba0-4e04-8fb2-89c50e52bd56"
   },
   "source": [
    "1.1 handling numerical null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866461eb-1a56-4ddc-b38c-50df88bc1329",
   "metadata": {
    "id": "866461eb-1a56-4ddc-b38c-50df88bc1329"
   },
   "source": [
    "## 2. Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf86038-4d70-44a4-b857-9d6662b28001",
   "metadata": {
    "id": "bdf86038-4d70-44a4-b857-9d6662b28001"
   },
   "source": [
    "2.1 Data merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb745a3-ec65-41ac-9650-9c78b4f6cbc0",
   "metadata": {
    "id": "fbb745a3-ec65-41ac-9650-9c78b4f6cbc0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define merge dataset function\n",
    "def merge_datasets(df1, df2, df3, key):\n",
    "\n",
    "    merged_df = pd.merge(df1, df2, on=key, how='inner')\n",
    "    merged_df = pd.merge(merged_df, df3, on=key, how='inner')\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647c3936-7d2a-48aa-82f5-368d0d42059f",
   "metadata": {
    "id": "647c3936-7d2a-48aa-82f5-368d0d42059f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = merge_datasets(demographic_train,vitals_train,labs_train,'patient_id')\n",
    "test = merge_datasets(demographic_test,vitals_test,labs_test,'patient_id')\n",
    "y_train = train['hospital_death']\n",
    "X_train = train.drop('hospital_death', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7131d588-c936-4c67-b4a9-9c27e63f3a5a",
   "metadata": {
    "id": "7131d588-c936-4c67-b4a9-9c27e63f3a5a"
   },
   "source": [
    "2.2 Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cf26de-9dd9-4bc6-a24c-0d8cdeccfc83",
   "metadata": {
    "id": "39cf26de-9dd9-4bc6-a24c-0d8cdeccfc83",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acfcb57-c705-466e-9e20-7f56c2d019b1",
   "metadata": {
    "id": "1acfcb57-c705-466e-9e20-7f56c2d019b1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get the numerical columns\n",
    "numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "# get the categorical columns\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3855a8a-657e-4c18-8491-1e7ea3990054",
   "metadata": {
    "id": "b3855a8a-657e-4c18-8491-1e7ea3990054",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Pipeline for numerical data\n",
    "Numerical_transformer = Pipeline(\n",
    "    steps = [\n",
    "        (\"imputer\", SimpleImputer(strategy=\"mean\")),\n",
    "        (\"scaler\", StandardScaler())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0efc23a3-6ecd-4093-b06e-2aa1686eb136",
   "metadata": {
    "id": "0efc23a3-6ecd-4093-b06e-2aa1686eb136",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Pipeline for categorical data\n",
    "Categorical_transofrmer = Pipeline(\n",
    "    steps = [\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"encoder\", OneHotEncoder())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b0ee5d-d1e0-4474-b25e-1284e1714438",
   "metadata": {
    "id": "d3b0ee5d-d1e0-4474-b25e-1284e1714438",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#preprocessor to transform data\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"numerical\", Numerical_transformer, numerical_features),\n",
    "        (\"categorical\", Categorical_transofrmer, categorical_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fdcf88-d2b0-49ec-8256-984a4eab94fe",
   "metadata": {
    "id": "58fdcf88-d2b0-49ec-8256-984a4eab94fe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "processor_pipeline = Pipeline(\n",
    "                steps=[('preprocessor',preprocessor)]\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9702833b-215d-43e4-ba50-a3f9981f23a7",
   "metadata": {
    "id": "9702833b-215d-43e4-ba50-a3f9981f23a7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#use merge dataset function\n",
    "train_processed = processor_pipeline.fit_transform(X_train)\n",
    "test_processed = processor_pipeline.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cacae8-f3c9-45b8-9261-fc1c07e9255a",
   "metadata": {
    "id": "08cacae8-f3c9-45b8-9261-fc1c07e9255a"
   },
   "source": [
    "3. Fit on models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8889158-178a-429f-8e53-e52d1a229bb5",
   "metadata": {
    "id": "c8889158-178a-429f-8e53-e52d1a229bb5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sampling data, straitify on the target column, also split group columns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class StratifiedGroupSampler:\n",
    "    def __init__(self, df, target_col, group_col, fraction, random_state=42):\n",
    "        self.df = df\n",
    "        self.target_col = target_col\n",
    "        self.group_col = group_col\n",
    "        self.fraction = fraction\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def sample(self):\n",
    "        sampled_df = pd.DataFrame()\n",
    "\n",
    "        for target in self.df[self.target_col].unique():\n",
    "            group_ids = self.df[self.df[self.target_col] == target][self.group_col].unique()\n",
    "            _, sampled_ids = train_test_split(group_ids, test_size=self.fraction, random_state=self.random_state)\n",
    "\n",
    "            sampled_data = self.df[self.df[self.group_col].isin(sampled_ids)]\n",
    "            sampled_df = pd.concat([sampled_df, sampled_data])\n",
    "\n",
    "        return sampled_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7411a14d-7473-4dad-9b4c-ed6237610a04",
   "metadata": {
    "id": "7411a14d-7473-4dad-9b4c-ed6237610a04",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_data(df, fraction):\n",
    "\n",
    "    sampler = StratifiedGroupSampler(df, 'hospital_death', 'patient_id', fraction)\n",
    "    sampled_train = sampler.sample()\n",
    "    X_sampled = sampled_train.drop('hospital_death', axis = 1)\n",
    "    y_sampled = sampled_train['hospital_death']\n",
    "\n",
    "\n",
    "    return X_sampled, y_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d0e66a-843b-4fc1-88c0-e03e516ecb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def sample_and_validate(pipeline, train_data, fraction):\n",
    "    # Sample and process the data\n",
    "    X_sampled, y_sampled = sample_data(train_data, fraction)\n",
    "    X_processed_sample = preprocessor.fit_transform(X_sampled)\n",
    "\n",
    "    # Define scoring dictionary\n",
    "    scoring = {\n",
    "        'precision': 'precision',\n",
    "        'recall': 'recall',\n",
    "        'f1_score': 'f1'\n",
    "    }\n",
    "\n",
    "    # Calculate cross-validation scores\n",
    "    cv_scores = cross_validate(pipeline, X_processed_sample, y_sampled, cv=5, scoring=scoring)\n",
    "\n",
    "    return cv_scores\n",
    "\n",
    "\n",
    "# Define the pipelines to try\n",
    "pipelines = [\n",
    "    {\"name\": \"Logistic Regression\", \"pipeline\": Pipeline([('classifier', LogisticRegression(max_iter=1000, class_weight='balanced'))]), \"params\": {'max_iter': 1000}},\n",
    "    {\"name\": \"Random Forest\", \"pipeline\": Pipeline([('classifier', RandomForestClassifier())]), \"params\": {}},\n",
    "    {\"name\": \"Gradient Boost\", \"pipeline\": Pipeline([('classifier', GradientBoostingClassifier())]), \"params\": {}},\n",
    "]\n",
    "\n",
    "# Define the fractions to try\n",
    "fractions = [0.1, 0.2, 0.3]\n",
    "\n",
    "# Initialize W&B\n",
    "wandb.init(project=\"EHR_record\")\n",
    "\n",
    "# Loop over the pipelines and fractions\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Loop over the pipelines and fractions\n",
    "for pipeline_dict in pipelines:\n",
    "    for fraction in fractions:\n",
    "        print(f\"Training {pipeline_dict['name']} with fraction {fraction}\")\n",
    "        X_sampled, y_sampled = sample_data(train_data, fraction)\n",
    "        X_processed_sample = preprocessor.fit_transform(X_sampled)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_processed_sample, y_sampled, test_size=0.2)\n",
    "\n",
    "        pipeline = pipeline_dict['pipeline']\n",
    "        pipeline.fit(X_train, y_train)\n",
    "        y_pred = pipeline.predict(X_test)\n",
    "        cv_scores = sample_and_validate(pipeline, train, fraction)\n",
    "\n",
    "        # Compute and plot the confusion matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        plt.figure(figsize=(10,7))\n",
    "        sns.heatmap(cm, annot=True, cmap='Blues')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title(f\"Confusion Matrix for {pipeline_dict['name']} with fraction {fraction}\")\n",
    "\n",
    "        # Log the plot to W&B\n",
    "        wandb.log({\n",
    "            \"confusion_matrix\": wandb.Image(plt),\n",
    "            \"precision\": cv_scores['test_precision'].mean(),\n",
    "            \"recall\": cv_scores['test_recall'].mean(),\n",
    "            \"f1_score\": cv_scores['test_f1_score'].mean(),\n",
    "            \"fraction\": fraction,\n",
    "            \"model_name\": pipeline_dict['name'],\n",
    "            **pipeline_dict['params']\n",
    "        })\n",
    "        plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c87b599-4ba6-466b-902e-8d8b4550b3a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 140
    },
    "id": "2c87b599-4ba6-466b-902e-8d8b4550b3a6",
    "outputId": "231e947d-96fd-434e-cc1d-abf7b435844f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_confusion_matrix(cv_scores):\n",
    "    # Calculate mean of confusion matrix components\n",
    "    tn_mean = np.mean(cv_scores['test_tn'])\n",
    "    fp_mean = np.mean(cv_scores['test_fp'])\n",
    "    fn_mean = np.mean(cv_scores['test_fn'])\n",
    "    tp_mean = np.mean(cv_scores['test_tp'])\n",
    "\n",
    "    # Create confusion matrix with mean values\n",
    "    cm = np.array([[tn_mean, fp_mean],\n",
    "                   [fn_mean, tp_mean]])\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='.0f', cmap='Blues',\n",
    "                xticklabels=['Predicted Negative', 'Predicted Positive'],\n",
    "                yticklabels=['Actual Negative', 'Actual Positive'])\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6016ae08-9237-4cb8-b374-9a4bf69640ac",
   "metadata": {
    "id": "6016ae08-9237-4cb8-b374-9a4bf69640ac"
   },
   "source": [
    "2.2 Check the linearality of predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085ed5d5-6b14-41b7-9b1c-8ddd597dad5e",
   "metadata": {
    "id": "085ed5d5-6b14-41b7-9b1c-8ddd597dad5e",
    "outputId": "b2693526-a6a0-4de5-fcb8-3635146a9c1a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = X_train.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "plt.savefig('correlation graph.png')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
