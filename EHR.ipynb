{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a62e09a-b16a-47cf-bc2a-83a764a4f698",
   "metadata": {
    "id": "8a62e09a-b16a-47cf-bc2a-83a764a4f698",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TIehMrYcjCwX",
   "metadata": {
    "id": "TIehMrYcjCwX"
   },
   "outputs": [],
   "source": [
    "#for colab reading files\n",
    "path = 'drive/My Drive/Projects/EHR_record/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vRN8rZYGjsmp",
   "metadata": {
    "id": "vRN8rZYGjsmp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "path=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160c1570-b05e-4bf8-a57c-f75d329e1ea3",
   "metadata": {
    "id": "160c1570-b05e-4bf8-a57c-f75d329e1ea3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "demographic_train = pd.read_csv(path+'Train/demographics.csv')\n",
    "labs_train = pd.read_csv(path+'Train/labs.csv')\n",
    "vitals_train = pd.read_csv(path+'Train/vitals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "G_32CmBHi56R",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_32CmBHi56R",
    "outputId": "60a5ef76-9d56-4a9f-8fd0-bc46cd91a24e"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b1088e-b2aa-40e2-af67-14a4e419d25a",
   "metadata": {
    "id": "e0b1088e-b2aa-40e2-af67-14a4e419d25a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "demographic_test = pd.read_csv(path+'Test/demographics.csv')\n",
    "labs_test = pd.read_csv(path+'Test/labs.csv')\n",
    "vitals_test = pd.read_csv(path+'Test/vitals.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1455804d-bc93-4fbb-b5e2-c74e82b203d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_20 = demographic_train.sample(20).to_csv('demo.csv')\n",
    "lab_20 = labs_train.sample(20).to_csv('labs.csv')\n",
    "vital_20 = vitals_train.sample(20).to_csv('vitals.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116c470f-6f60-4181-bb78-caa467b4b29d",
   "metadata": {
    "id": "116c470f-6f60-4181-bb78-caa467b4b29d"
   },
   "source": [
    "## 1. Exloratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f58f201-095b-4c86-9701-daa89deb8158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_eda(dataframes, names):\n",
    "    for df, name in zip(dataframes, names):\n",
    "        print(f\"EDA for {name}:\")\n",
    "        print(df.describe())\n",
    "        print(f\"Size of {name}:\", df.shape)\n",
    "        print(f\"Null values in {name}:\\n\", df.isnull().sum())\n",
    "        print(\"\\n\")\n",
    "\n",
    "# Usage:\n",
    "dataframes = [demographic_train, labs_train, vitals_train]\n",
    "names = [\"demographic\", \"labs\", \"vitals\"]\n",
    "perform_eda(dataframes, names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb745a3-ec65-41ac-9650-9c78b4f6cbc0",
   "metadata": {
    "id": "fbb745a3-ec65-41ac-9650-9c78b4f6cbc0",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define merge dataset function\n",
    "def merge_datasets(df1, df2, df3, key):\n",
    "\n",
    "    merged_df = pd.merge(df1, df2, on=key, how='inner')\n",
    "    merged_df = pd.merge(merged_df, df3, on=key, how='inner')\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647c3936-7d2a-48aa-82f5-368d0d42059f",
   "metadata": {
    "id": "647c3936-7d2a-48aa-82f5-368d0d42059f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = merge_datasets(demographic_train,vitals_train,labs_train,'patient_id')\n",
    "test = merge_datasets(demographic_test,vitals_test,labs_test,'patient_id')\n",
    "y_train = train['hospital_death']\n",
    "X_train = train.drop('hospital_death', axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866461eb-1a56-4ddc-b38c-50df88bc1329",
   "metadata": {
    "id": "866461eb-1a56-4ddc-b38c-50df88bc1329"
   },
   "source": [
    "## 2. Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cf26de-9dd9-4bc6-a24c-0d8cdeccfc83",
   "metadata": {
    "id": "39cf26de-9dd9-4bc6-a24c-0d8cdeccfc83",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cacae8-f3c9-45b8-9261-fc1c07e9255a",
   "metadata": {
    "id": "08cacae8-f3c9-45b8-9261-fc1c07e9255a"
   },
   "source": [
    "3. Fit on models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8889158-178a-429f-8e53-e52d1a229bb5",
   "metadata": {
    "id": "c8889158-178a-429f-8e53-e52d1a229bb5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sampling data, straitify on the target column, also split group columns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class StratifiedGroupSampler:\n",
    "    def __init__(self, df, target_col, group_col, fraction, random_state=42):\n",
    "        self.df = df\n",
    "        self.target_col = target_col\n",
    "        self.group_col = group_col\n",
    "        self.fraction = fraction\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def sample(self):\n",
    "        sampled_df = pd.DataFrame()\n",
    "\n",
    "        for target in self.df[self.target_col].unique():\n",
    "            group_ids = self.df[self.df[self.target_col] == target][self.group_col].unique()\n",
    "            _, sampled_ids = train_test_split(group_ids, test_size=self.fraction, random_state=self.random_state)\n",
    "\n",
    "            sampled_data = self.df[self.df[self.group_col].isin(sampled_ids)]\n",
    "            sampled_df = pd.concat([sampled_df, sampled_data])\n",
    "\n",
    "        return sampled_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873014e6-371e-49a0-b05d-4706aeac624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(df, fraction):\n",
    "\n",
    "    sampler = StratifiedGroupSampler(df, 'hospital_death', 'patient_id', fraction)\n",
    "    sampled_train = sampler.sample()\n",
    "    X_sampled = sampled_train.drop('hospital_death', axis = 1)\n",
    "    y_sampled = sampled_train['hospital_death']\n",
    "\n",
    "\n",
    "    return X_sampled, y_sampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935eea25-db90-4234-afa2-43f3cf8d294d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Random Forest hyperparameters\n",
    "rf_params = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__max_depth': [10, 20, 30],\n",
    "    'classifier__min_samples_split': [2, 5, 10],\n",
    "    'classifier__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Gradient Boosting hyperparameters\n",
    "gb_params = {\n",
    "    'classifier__n_estimators': [100, 200, 300],\n",
    "    'classifier__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'classifier__max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82f0970-9b29-41d6-a863-5a9f345af16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical features\n",
    "numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Get categorical features\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "\n",
    "# Define the preprocessing steps with all pipeline\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine numerical and categorical preprocessing\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Define the pipelines to try\n",
    "pipelines = [\n",
    "    {\"name\": \"Logistic Regression\", \"pipeline\": Pipeline([('preprocessor', preprocessor),\n",
    "                                                          ('classifier', LogisticRegression(max_iter=1000, class_weight='balanced'))]), \n",
    "                                                            \"params_grid\": {'max_iter': [100, 500, 1000]}},\n",
    "   {\"name\": \"Random Forest\", \"pipeline\": Pipeline([('preprocessor', preprocessor),\n",
    "                                                   ('classifier', RandomForestClassifier())]), \n",
    "                                                    \"params_grid\": rf_params},\n",
    "    \n",
    "    {\"name\": \"Gradient Boost\", \"pipeline\": Pipeline([('preprocessor', preprocessor),\n",
    "                                                     ('classifier', GradientBoostingClassifier())]),\n",
    "                                                     \"params_grid\": gb_params},\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5179ec-5f87-4a7d-826b-57e5ee2cd78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d0e66a-843b-4fc1-88c0-e03e516ecb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define the fractions to try\n",
    "fractions = [0.1, 0.2, 0.3]\n",
    "\n",
    "# Define scoring dictionary\n",
    "scoring = {\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'f1_score': 'f1'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add66de6-8f74-4674-aada-54c2c990ecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Helper function to fit the model with error handling\n",
    "def fit_model(pipeline_dict, X_train, y_train):\n",
    "    try:\n",
    "        random_search = RandomizedSearchCV(pipeline_dict['pipeline'], pipeline_dict.get('params_grid', {}),\n",
    "                                           n_iter=10, cv=5, scoring=scoring, random_state=42)\n",
    "        random_search.fit(X_train, y_train)\n",
    "        return random_search\n",
    "    except Exception as e:\n",
    "        error_message = f\"An error occurred while fitting the model {pipeline_dict['name']}: {e}\"\n",
    "        print(error_message)\n",
    "        wandb.log({\"error\": error_message}) # Logging the error to W&B\n",
    "        return None\n",
    "\n",
    "# Helper function to evaluate the model with error handling\n",
    "def evaluate_model(random_search, X_val, y_val, model_name, fraction):\n",
    "    if random_search is None:\n",
    "        error_message = f\"Skipping evaluation for {model_name} due to an error during fitting.\"\n",
    "        print(error_message)\n",
    "        wandb.log({\"error\": error_message}) # Logging the error to W&B\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        y_pred = random_search.predict(X_val)\n",
    "        y_prob = random_search.predict_proba(X_val)[:, 1]\n",
    "        cm = confusion_matrix(y_val, y_pred)\n",
    "        fpr, tpr, _ = roc_curve(y_val, y_prob)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        log_results(random_search, cm, fpr, tpr, roc_auc, model_name, fraction)\n",
    "    except Exception as e:\n",
    "        error_message = f\"An error occurred while evaluating the model {model_name}: {e}\"\n",
    "        print(error_message)\n",
    "        wandb.log({\"error\": error_message}) # Logging the error to W&B\n",
    "\n",
    "def log_results(random_search, cm, fpr, tpr, roc_auc, model_name, fraction):\n",
    "    \"\"\"Log results to W&B, including confusion matrix, ROC curve, and feature importances.\"\"\"\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(cm, annot=True, cmap='Blues')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f\"Confusion Matrix for {model_name} with fraction {fraction}\")\n",
    "    confusion_matrix_image = plt.gcf()\n",
    "    plt.close()\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(fpr, tpr, lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], lw=2, linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve for {model_name} with fraction {fraction}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    roc_curve_image = plt.gcf()\n",
    "    plt.close()\n",
    "\n",
    "    # Log the results\n",
    "    wandb.log({\n",
    "        \"model_name\": model_name,\n",
    "        \"best_params\": random_search.best_params_,\n",
    "        \"best_score\": random_search.best_score_,\n",
    "        \"confusion_matrix\": wandb.Image(confusion_matrix_image),\n",
    "        \"roc_curve\": wandb.Image(roc_curve_image),\n",
    "        \"precision\": random_search.cv_results_['mean_test_precision'].mean(),\n",
    "        \"recall\": random_search.cv_results_['mean_test_recall'].mean(),\n",
    "        \"f1_score\": random_search.cv_results_['mean_test_f1_score'].mean(),\n",
    "        \"fraction\": fraction\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa98153a-abbd-4be5-b33a-0827f11fa83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(pipelines, fractions, train_data):\n",
    "    # Initialize W&B\n",
    "    wandb.init(project=\"EHR_record\")\n",
    "\n",
    "    # Loop over the pipelines and fractions\n",
    "    for pipeline_dict in pipelines:\n",
    "        for fraction in fractions:\n",
    "            print(f\"Training {pipeline_dict['name']} with fraction {fraction}\")\n",
    "            X_sampled, y_sampled = sample_data(train_data, fraction)\n",
    "            X_train, X_val, y_train, y_val = train_test_split(X_sampled, y_sampled, test_size=0.2)\n",
    "\n",
    "            # Fit the model\n",
    "            random_search = fit_model(pipeline_dict, X_train, y_train)\n",
    "\n",
    "            # Evaluate the model\n",
    "            evaluate_model(random_search, X_val, y_val, pipeline_dict['name'], fraction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01f1d77-70ac-421e-a279-1313f2f43bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Train and evaluate model\n",
    "train_and_evaluate(pipelines, fractions, train)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "data",
   "language": "python",
   "name": "data"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
